      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unbalanced panel with two types of missingness\n",
        "data_unbalanced = data.copy()\n",
        "\n",
        "# Type 1: Random missingness (10% of observations)\n",
        "random_drop = np.random.random(len(data)) < 0.1\n",
        "data_unbalanced_random = data[~random_drop].copy()\n",
        "\n",
        "# Type 2: Systematic attrition (some firms exit in later periods)\n",
        "# Firms with lower profits are more likely to exit\n",
        "entity_avg_profit = data.groupby('entity_id')['profit'].mean()\n",
        "bottom_quartile = entity_avg_profit.quantile(0.25)\n",
        "struggling_firms = entity_avg_profit[entity_avg_profit < bottom_quartile].index\n",
        "\n",
        "# These firms exit after period 5\n",
        "systematic_drop = (data['entity_id'].isin(struggling_firms)) & (data['time'] > 5)\n",
        "data_unbalanced_systematic = data[~systematic_drop].copy()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"UNBALANCED PANEL ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Original observations: {len(data)}\")\n",
        "print(f\"\\nRandom missingness (10%):\")\n",
        "print(f\"  Remaining observations: {len(data_unbalanced_random)}\")\n",
        "obs_per_entity_random = data_unbalanced_random.groupby('entity_id').size()\n",
        "print(f\"  Obs per entity - Min: {obs_per_entity_random.min()}, Max: {obs_per_entity_random.max()}, Mean: {obs_per_entity_random.mean():.2f}\")\n",
        "\n",
        "print(f\"\\nSystematic attrition (low-profit firms exit):\")\n",
        "print(f\"  Remaining observations: {len(data_unbalanced_systematic)}\")\n",
        "print(f\"  Number of firms that exit early: {len(struggling_firms)}\")\n",
        "obs_per_entity_systematic = data_unbalanced_systematic.groupby('entity_id').size()\n",
        "print(f\"  Obs per entity - Min: {obs_per_entity_systematic.min()}, Max: {obs_per_entity_systematic.max()}, Mean: {obs_per_entity_systematic.mean():.2f}\")\n",
        "\n",
        "# Fit FE models on both types of unbalanced panels\n",
        "for panel_type, panel_data in [('Random Missing', data_unbalanced_random),\n",
        "                               ('Systematic Attrition', data_unbalanced_systematic)]:\n",
        "    \n",
        "    panel_indexed = panel_data.set_index(['entity_id', 'time'])\n",
        "    y_unbal = panel_indexed['profit']\n",
        "    X_unbal = panel_indexed[['investment', 'market_conditions', 'firm_size']]\n",
        "    \n",
        "    fe_unbal = PanelOLS(y_unbal, X_unbal, entity_effects=True)\n",
        "    fe_unbal_results = fe_unbal.fit(cov_type='clustered', cluster_entity{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fixed Effects Models in Python: A Comprehensive Guide\n",
        "\n",
        "## What You Will Learn\n",
        "- ✅ Understand when fixed effects (FE) models are appropriate vs random effects or pooled OLS\n",
        "- ✅ Implement FE models using both `linearmodels` (recommended) and `statsmodels`\n",
        "- ✅ Perform essential diagnostic tests (Hausman test, serial correlation, heteroskedasticity)\n",
        "- ✅ Handle common pitfalls (time-invariant variables, unbalanced panels, small-T bias)\n",
        "- ✅ Apply clustered standard errors and understand when different types are needed\n",
        "\n",
        "## Prerequisites\n",
        "- Basic Python and pandas knowledge\n",
        "- Understanding of linear regression\n",
        "- Familiarity with panel data structure (entities observed over time)\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction to Fixed Effects](#intro)\n",
        "2. [When to Use Fixed Effects vs Random Effects](#when-to-use)\n",
        "3. [Data Preparation](#data-prep)\n",
        "4. [Implementation with linearmodels](#linearmodels)\n",
        "5. [Implementation with statsmodels](#statsmodels)\n",
        "6. [Model Diagnostics and Testing](#diagnostics)\n",
        "7. [Advanced Topics](#advanced)\n",
        "8. [Common Pitfalls and Solutions](#pitfalls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick example with real wage panel data\n",
        "try:\n",
        "    from linearmodels.datasets import wage_panel\n",
        "    wages = wage_panel.load()\n",
        "    \n",
        "    print(\"Wage Panel Data Overview:\")\n",
        "    print(f\"Shape: {wages.shape}\")\n",
        "    print(f\"Variables: {list(wages.columns)}\")\n",
        "    print(f\"\\nFirst few observations:\")\n",
        "    print(wages.head())\n",
        "    \n",
        "    # Quick FE regression: log wages on experience and education\n",
        "    wages_indexed = wages.set_index(['nr', 'year'])\n",
        "    \n",
        "    # Note: education is time-invariant, will be dropped in FE\n",
        "    wage_fe = PanelOLS(wages_indexed['lwage'], \n",
        "                       wages_indexed[['expersq', 'married', 'union']],\n",
        "                       entity_effects=True)\n",
        "    wage_results = wage_fe.fit(cov_type='clustered', cluster_entity=True)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"WAGE FIXED EFFECTS RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(wage_results.summary.tables[1])\n",
        "    print(\"\\nNote: Education dropped due to no within-person variation\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"Real data example requires linearmodels datasets\")\n",
        "    print(\"This is already installed if you have linearmodels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print execution environment info\n",
        "import sys\n",
        "import importlib.metadata\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"NOTEBOOK EXECUTION INFORMATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Executed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Python version: {sys.version.split()[0]}\")\n",
        "print(\"\\nKey package versions:\")\n",
        "for package in ['linearmodels', 'statsmodels', 'pandas', 'numpy', 'scipy']:\n",
        "    try:\n",
        "        version = importlib.metadata.version(package)\n",
        "        print(f\"  {package}: {version}\")\n",
        "    except:\n",
        "        print(f\"  {package}: not installed\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"For the most up-to-date version of this notebook, see:\")\n",
        "print(\"[Link to GitHub repository]\")\n",
        "print(\"\\nFor solutions to exercises, see:\")\n",
        "print(\"[Link to solutions gist]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Fixed Effects <a id='intro'></a>\n",
        "\n",
        "### What are Fixed Effects Models?\n",
        "Fixed effects (FE) models are a class of statistical models used to analyze panel data (repeated observations over time for the same entities). They control for time-invariant unobserved heterogeneity by allowing each entity to have its own intercept.\n",
        "\n",
        "### Mathematical Representation\n",
        "The basic fixed effects model can be written as:\n",
        "\n",
        "$$y_{it} = \\alpha_i + \\beta X_{it} + \\epsilon_{it}$$\n",
        "\n",
        "Where:\n",
        "- $y_{it}$ is the dependent variable for entity $i$ at time $t$\n",
        "- $\\alpha_i$ is the entity-specific intercept (fixed effect)\n",
        "- $X_{it}$ is a vector of time-varying predictors\n",
        "- $\\beta$ is the coefficient vector\n",
        "- $\\epsilon_{it}$ is the error term\n",
        "\n",
        "### Key Assumptions\n",
        "1. **Strict exogeneity**: $E[\\epsilon_{it}|X_i, \\alpha_i] = 0$\n",
        "2. **No perfect multicollinearity**\n",
        "3. **Large N assumption**: Number of entities should be reasonably large\n",
        "4. **Time-invariant unobserved effects**: $\\alpha_i$ doesn't vary with time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. When to Use Fixed Effects vs Random Effects <a id='when-to-use'></a>\n",
        "\n",
        "### Fixed Effects are preferred when:\n",
        "- You believe unobserved heterogeneity is correlated with predictors\n",
        "- You're interested in within-entity variation\n",
        "- You have a relatively small number of time periods\n",
        "\n",
        "### Random Effects are preferred when:\n",
        "- Unobserved heterogeneity is uncorrelated with predictors\n",
        "- You're interested in both within and between variation\n",
        "- You have time-invariant predictors of interest\n",
        "\n",
        "### The Hausman Test\n",
        "We'll implement the Hausman test later to help choose between FE and RE models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setup and Data Preparation <a id='data-prep'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First time setup: save this as requirements.txt and run:\n",
        "# pip install -r requirements.txt\n",
        "\"\"\"\n",
        "linearmodels>=4.27\n",
        "statsmodels>=0.13.0\n",
        "pandas>=1.3.0\n",
        "numpy>=1.21.0\n",
        "matplotlib>=3.4.0\n",
        "seaborn>=0.11.0\n",
        "scipy>=1.7.0\n",
        "\"\"\"\n",
        "\n",
        "# For this notebook, uncomment the line below if packages aren't installed:\n",
        "# !pip install linearmodels>=4.27 statsmodels pandas numpy matplotlib seaborn scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import f  # For F-distribution\n",
        "from linearmodels.panel import PanelOLS, RandomEffects, compare\n",
        "from linearmodels.panel.data import PanelData\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "import warnings\n",
        "\n",
        "# Only filter specific warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', message='DataFrame.groupby with axis=1')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "plt.style.use('seaborn-darkgrid')  # Updated to avoid deprecation\n",
        "\n",
        "# Set random seed once for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a More Realistic Dataset\n",
        "Let's create a dataset that better demonstrates the need for fixed effects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n_entities = 50  # Number of firms\n",
        "n_periods = 10   # Number of time periods\n",
        "n_obs = n_entities * n_periods\n",
        "\n",
        "# True coefficients (we'll compare our estimates to these)\n",
        "TRUE_BETAS = {\n",
        "    'investment': 1.5,\n",
        "    'market_conditions': 0.8,\n",
        "    'firm_size': 0.5\n",
        "}\n",
        "\n",
        "# Generate panel structure\n",
        "entities = np.repeat(range(n_entities), n_periods)\n",
        "time = np.tile(range(n_periods), n_entities)\n",
        "\n",
        "# Generate entity fixed effects (unobserved ability/quality)\n",
        "entity_effects = np.random.normal(0, 2, n_entities)\n",
        "entity_effects_expanded = np.repeat(entity_effects, n_periods)\n",
        "\n",
        "# Add time-invariant characteristic (sector) before generating other variables\n",
        "sectors = ['Tech', 'Finance', 'Manufacturing']\n",
        "entity_sectors = np.random.choice(sectors, n_entities)\n",
        "sector_expanded = np.repeat(entity_sectors, n_periods)\n",
        "\n",
        "# Generate time-varying variables\n",
        "# X1: Investment (correlated with entity effects - this creates endogeneity!)\n",
        "x1 = 5 + 0.5 * entity_effects_expanded + np.random.normal(0, 1, n_obs)\n",
        "\n",
        "# X2: Market conditions (common to all, varies by time)\n",
        "time_trend = np.tile(np.linspace(0, 2, n_periods), n_entities)\n",
        "x2 = 3 + time_trend + np.random.normal(0, 0.5, n_obs)\n",
        "\n",
        "# X3: Firm size (slowly changing)\n",
        "x3 = 10 + 0.3 * entity_effects_expanded + 0.1 * time_trend + np.random.normal(0, 0.3, n_obs)\n",
        "\n",
        "# Generate heteroskedastic errors (variance depends on firm size)\n",
        "error_variance = 0.5 + 0.1 * x3  # Larger firms have more variable profits\n",
        "errors = np.random.normal(0, np.sqrt(error_variance))\n",
        "\n",
        "# Generate dependent variable using true model\n",
        "y = (2 + entity_effects_expanded + \n",
        "     TRUE_BETAS['investment'] * x1 + \n",
        "     TRUE_BETAS['market_conditions'] * x2 + \n",
        "     TRUE_BETAS['firm_size'] * x3 + \n",
        "     errors)\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'entity_id': entities,\n",
        "    'time': time,\n",
        "    'profit': y,\n",
        "    'investment': x1,\n",
        "    'market_conditions': x2,\n",
        "    'firm_size': x3,\n",
        "    'sector': sector_expanded\n",
        "})\n",
        "\n",
        "# Add entity names for readability\n",
        "data['entity'] = 'Firm_' + data['entity_id'].astype(str)\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"\\nNumber of entities: {data['entity_id'].nunique()}\")\n",
        "print(f\"Number of time periods: {data['time'].nunique()}\")\n",
        "print(f\"\\nTrue coefficients:\")\n",
        "for var, coef in TRUE_BETAS.items():\n",
        "    print(f\"  {var}: {coef}\")\n",
        "print(f\"\\nNote: Data includes heteroskedastic errors (variance increases with firm size)\")\n",
        "print(f\"\\nFirst few observations:\")\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"Summary Statistics:\")\n",
        "print(data[['profit', 'investment', 'market_conditions', 'firm_size']].describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values: {data.isnull().sum().sum()}\")\n",
        "\n",
        "# Check panel balance\n",
        "panel_balance = data.groupby('entity_id')['time'].count()\n",
        "print(f\"\\nPanel balance - all entities have {n_periods} observations: {(panel_balance == n_periods).all()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize heterogeneity across entities\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle('Panel Data Heterogeneity Analysis', fontsize=16, y=0.98)\n",
        "plt.subplots_adjust(top=0.92, hspace=0.3)\n",
        "\n",
        "# Plot 1: Profit over time for different entities using seaborn\n",
        "sample_entities = np.random.choice(data['entity_id'].unique(), 5, replace=False)\n",
        "sample_data = data[data['entity_id'].isin(sample_entities)]\n",
        "sns.lineplot(data=sample_data, x='time', y='profit', hue='entity', \n",
        "             marker='o', ax=axes[0, 0], legend='brief')\n",
        "axes[0, 0].set_xlabel('Time')\n",
        "axes[0, 0].set_ylabel('Profit')\n",
        "axes[0, 0].set_title('Profit Trends for Sample Firms')\n",
        "\n",
        "# Plot 2: Distribution of profits by entity\n",
        "entity_means = data.groupby('entity_id')['profit'].mean().sort_values()\n",
        "axes[0, 1].bar(range(len(entity_means)), entity_means.values, alpha=0.7)\n",
        "axes[0, 1].set_xlabel('Entity (sorted by mean profit)')\n",
        "axes[0, 1].set_ylabel('Mean Profit')\n",
        "axes[0, 1].set_title('Heterogeneity in Average Profits')\n",
        "axes[0, 1].axhline(y=entity_means.mean(), color='red', linestyle='--', \n",
        "                   label=f'Grand mean: {entity_means.mean():.2f}')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Plot 3: Within-entity correlation matrix (entity-demeaned variables)\n",
        "vars_for_corr = ['profit', 'investment', 'market_conditions', 'firm_size']\n",
        "data_demeaned_temp = data.copy()\n",
        "for var in vars_for_corr:\n",
        "    entity_means_temp = data.groupby('entity_id')[var].transform('mean')\n",
        "    data_demeaned_temp[f'{var}_within'] = data[var] - entity_means_temp\n",
        "\n",
        "within_corr = data_demeaned_temp[[f'{v}_within' for v in vars_for_corr]].corr()\n",
        "within_corr.columns = vars_for_corr\n",
        "within_corr.index = vars_for_corr\n",
        "\n",
        "sns.heatmap(within_corr, annot=True, cmap='coolwarm', center=0, ax=axes[1, 0],\n",
        "            fmt='.3f', square=True)\n",
        "axes[1, 0].set_title('Within-Entity Correlations')\n",
        "\n",
        "# Plot 4: Between vs Within variation\n",
        "entity_means_df = data.groupby('entity_id')[['profit', 'investment']].mean()\n",
        "entity_means_df.columns = ['profit_mean', 'investment_mean']\n",
        "\n",
        "# Between variation (entity means)\n",
        "axes[1, 1].scatter(entity_means_df['investment_mean'], \n",
        "                   entity_means_df['profit_mean'], \n",
        "                   alpha=0.6, s=50, label='Between variation')\n",
        "\n",
        "# Add regression line\n",
        "z = np.polyfit(entity_means_df['investment_mean'], entity_means_df['profit_mean'], 1)\n",
        "p = np.poly1d(z)\n",
        "axes[1, 1].plot(entity_means_df['investment_mean'].sort_values(), \n",
        "                p(entity_means_df['investment_mean'].sort_values()), \n",
        "                \"r--\", alpha=0.8, label=f'Slope: {z[0]:.2f}')\n",
        "\n",
        "axes[1, 1].set_xlabel('Investment (Entity Means)')\n",
        "axes[1, 1].set_ylabel('Profit (Entity Means)')\n",
        "axes[1, 1].set_title('Between-Entity Relationship')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fixed Effects with linearmodels <a id='linearmodels'></a>\n",
        "\n",
        "The `linearmodels` package is specifically designed for panel data analysis and provides efficient implementations of various panel data models.\n",
        "\n",
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_panel_model(data, y_col, x_cols, model_type='fe', entity_effects=True, \n",
        "                   time_effects=False, cov_type='clustered', **cov_kwargs):\n",
        "    \"\"\"\n",
        "    Fit panel data model with specified options.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame with MultiIndex (entity, time)\n",
        "    y_col : str, dependent variable column name\n",
        "    x_cols : list, independent variable column names\n",
        "    model_type : str, 'fe' for fixed effects, 're' for random effects\n",
        "    entity_effects : bool, include entity fixed effects\n",
        "    time_effects : bool, include time fixed effects\n",
        "    cov_type : str, covariance type for standard errors\n",
        "    **cov_kwargs : additional arguments for covariance estimation\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    Fitted model results\n",
        "    \"\"\"\n",
        "    y = data[y_col]\n",
        "    X = data[x_cols]\n",
        "    \n",
        "    if model_type == 'fe':\n",
        "        model = PanelOLS(y, X, entity_effects=entity_effects, time_effects=time_effects)\n",
        "    elif model_type == 're':\n",
        "        model = RandomEffects(y, X)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
        "    \n",
        "    # Set default clustering for FE models\n",
        "    if cov_type == 'clustered' and 'cluster_entity' not in cov_kwargs:\n",
        "        cov_kwargs['cluster_entity'] = True\n",
        "    \n",
        "    return model.fit(cov_type=cov_type, **cov_kwargs)\n",
        "\n",
        "# Prepare data for linearmodels\n",
        "# Set multi-index (entity, time)\n",
        "data_panel = data.set_index(['entity_id', 'time'])\n",
        "\n",
        "# Define common parameters for later comparisons\n",
        "common_params = ['investment', 'market_conditions', 'firm_size']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Entity Fixed Effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit entity fixed effects model using helper function\n",
        "fe_entity_results = fit_panel_model(data_panel, 'profit', common_params, \n",
        "                                   model_type='fe', entity_effects=True, \n",
        "                                   time_effects=False)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ENTITY FIXED EFFECTS MODEL\")\n",
        "print(\"=\" * 80)\n",
        "print(fe_entity_results)\n",
        "\n",
        "# Compare with true values\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COEFFICIENT COMPARISON WITH TRUE VALUES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Variable':<20} {'True':<10} {'Estimated':<10} {'Bias':<10} {'Bias %':<10}\")\n",
        "print(\"-\" * 70)\n",
        "for var in common_params:\n",
        "    true_val = TRUE_BETAS[var]\n",
        "    est_val = fe_entity_results.params[var]\n",
        "    bias = est_val - true_val\n",
        "    bias_pct = (bias / true_val) * 100\n",
        "    print(f\"{var:<20} {true_val:<10.3f} {est_val:<10.3f} {bias:<10.3f} {bias_pct:<10.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Two-Way Fixed Effects (Entity + Time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit two-way fixed effects model (entity + time)\n",
        "fe_twoway_results = fit_panel_model(data_panel, 'profit', common_params,\n",
        "                                   model_type='fe', entity_effects=True,\n",
        "                                   time_effects=True)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TWO-WAY FIXED EFFECTS MODEL (Entity + Time)\")\n",
        "print(\"=\" * 80)\n",
        "print(fe_twoway_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Comparison with Pooled OLS and Random Effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pooled OLS (no effects)\n",
        "pooled_results = fit_panel_model(data_panel, 'profit', common_params,\n",
        "                                model_type='fe', entity_effects=False,\n",
        "                                time_effects=False)\n",
        "\n",
        "# Random Effects\n",
        "re_results = fit_panel_model(data_panel, 'profit', common_params,\n",
        "                           model_type='re')\n",
        "\n",
        "# Compare models\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "comparison = compare({'Pooled OLS': pooled_results,\n",
        "                     'Random Effects': re_results,\n",
        "                     'Entity FE': fe_entity_results,\n",
        "                     'Two-way FE': fe_twoway_results})\n",
        "print(comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fixed Effects with statsmodels <a id='statsmodels'></a>\n",
        "\n",
        "While `statsmodels` is less efficient for large panels, it offers more flexibility and detailed diagnostic outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Using dummy variables (not recommended for large panels)\n",
        "# Demonstrate why this approach is problematic\n",
        "\n",
        "if n_entities <= 10:  # Only run for small datasets\n",
        "    formula = 'profit ~ investment + market_conditions + firm_size + C(entity_id)'\n",
        "    \n",
        "    # Time the execution\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    fe_sm_dummy = smf.ols(formula, data=data).fit()\n",
        "    dummy_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"Execution time: {dummy_time:.3f} seconds\")\n",
        "    print(f\"Number of parameters estimated: {len(fe_sm_dummy.params)}\")\n",
        "    print(\"\\nFixed Effects using Dummy Variables:\")\n",
        "    print(fe_sm_dummy.summary())\n",
        "else:\n",
        "    # Demonstrate memory issue\n",
        "    n_params_with_dummies = 3 + n_entities  # 3 regressors + entity dummies\n",
        "    memory_usage_mb = (n_obs * n_params_with_dummies * 8) / (1024**2)  # 8 bytes per float\n",
        "    \n",
        "    print(f\"⚠️ Skipping dummy variable approach due to large number of entities ({n_entities})\")\n",
        "    print(f\"\\nWhy this is problematic:\")\n",
        "    print(f\"• Would create {n_entities} dummy variables\")\n",
        "    print(f\"• Design matrix would have {n_obs} × {n_params_with_dummies} = {n_obs * n_params_with_dummies:,} elements\")\n",
        "    print(f\"• Approximate memory usage: {memory_usage_mb:.1f} MB just for the design matrix\")\n",
        "    print(f\"\\n→ Use within-transformation or specialized panel data methods instead!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Within transformation (demeaning)\n",
        "# This is more efficient and what FE essentially does\n",
        "\n",
        "def demean_panel_data(df, variables, entity_col='entity_id'):\n",
        "    \"\"\"Demean variables within entities for fixed effects estimation.\"\"\"\n",
        "    df_demeaned = df.copy()\n",
        "    for var in variables:\n",
        "        entity_means = df.groupby(entity_col)[var].transform('mean')\n",
        "        df_demeaned[f'{var}_dm'] = df[var] - entity_means\n",
        "    return df_demeaned\n",
        "\n",
        "# Apply demeaning\n",
        "variables = ['profit', 'investment', 'market_conditions', 'firm_size']\n",
        "data_demeaned = demean_panel_data(data, variables)\n",
        "\n",
        "# Fit OLS on demeaned data\n",
        "formula_dm = 'profit_dm ~ investment_dm + market_conditions_dm + firm_size_dm - 1'\n",
        "fe_sm_within = smf.ols(formula_dm, data=data_demeaned).fit()\n",
        "\n",
        "# Get clustered standard errors\n",
        "fe_sm_within_clustered = fe_sm_within.get_robustcov_results(\n",
        "    cov_type='cluster',\n",
        "    groups=data_demeaned['entity_id']\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FIXED EFFECTS USING WITHIN TRANSFORMATION (with clustered SEs)\")\n",
        "print(\"=\" * 80)\n",
        "print(fe_sm_within_clustered.summary())\n",
        "\n",
        "# Compare coefficients with true values\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COEFFICIENT COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Variable':<20} {'True':<10} {'Estimated':<10} {'Difference':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for var in ['investment', 'market_conditions', 'firm_size']:\n",
        "    true_val = TRUE_BETAS[var]\n",
        "    est_val = fe_sm_within_clustered.params[f'{var}_dm']\n",
        "    diff = est_val - true_val\n",
        "    print(f\"{var:<20} {true_val:<10.3f} {est_val:<10.3f} {diff:<10.3f}\")\n",
        "\n",
        "print(\"\\nNote: Standard errors are adjusted for clustering at the entity level.\")\n",
        "print(\"Degrees of freedom correction for demeaning is not automatic in statsmodels.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Diagnostics and Testing <a id='diagnostics'></a>\n",
        "\n",
        "Proper model diagnostics are crucial for ensuring the validity of your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Hausman Test (Fixed vs Random Effects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use linearmodels built-in comparison which includes Hausman test\n",
        "print(\"=\" * 80)\n",
        "print(\"HAUSMAN TEST USING LINEARMODELS COMPARE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# The compare function automatically performs the Hausman test\n",
        "comparison_with_test = compare({'Random Effects': re_results,\n",
        "                               'Entity FE': fe_entity_results})\n",
        "print(comparison_with_test)\n",
        "\n",
        "# Extract Hausman test results if available\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"INTERPRETATION:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"If the Hausman test p-value < 0.05:\")\n",
        "print(\"  → Reject H0: Fixed effects is preferred (RE estimates are inconsistent)\")\n",
        "print(\"If the Hausman test p-value ≥ 0.05:\")\n",
        "print(\"  → Fail to reject H0: Random effects may be appropriate\")\n",
        "print(\"\\nNote: The test assumes RE is efficient under H0 and tests if FE and RE estimates differ significantly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Testing for Time Fixed Effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# F-test for time fixed effects\n",
        "# Compare entity FE model with two-way FE model\n",
        "\n",
        "# Get R-squared values\n",
        "r2_entity = fe_entity_results.rsquared\n",
        "r2_twoway = fe_twoway_results.rsquared\n",
        "\n",
        "# Calculate F-statistic\n",
        "n_time_effects = n_periods - 1  # Number of time dummies\n",
        "n_obs_total = len(data_panel)\n",
        "n_params_twoway = len(fe_twoway_results.params) + n_entities + n_time_effects\n",
        "\n",
        "f_stat_time = ((r2_twoway - r2_entity) / n_time_effects) / ((1 - r2_twoway) / (n_obs_total - n_params_twoway))\n",
        "# Use scipy.stats.f that we imported at the top\n",
        "f_pvalue_time = 1 - f.cdf(f_stat_time, n_time_effects, n_obs_total - n_params_twoway)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TEST FOR TIME FIXED EFFECTS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"H0: Time fixed effects are jointly zero\")\n",
        "print(f\"H1: Time fixed effects are significant\")\n",
        "print(f\"\\nF-statistic: {f_stat_time:.4f}\")\n",
        "print(f\"P-value: {f_pvalue_time:.4f}\")\n",
        "print(f\"\\nConclusion: {'Reject H0 - Include time fixed effects' if f_pvalue_time < 0.05 else 'Fail to reject H0 - Time fixed effects may not be necessary'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Residual Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get residuals from the entity FE model\n",
        "residuals = fe_entity_results.resids\n",
        "\n",
        "# Create diagnostic plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle('Fixed Effects Model Diagnostics', fontsize=16, y=0.98)\n",
        "plt.subplots_adjust(top=0.92, hspace=0.3)\n",
        "\n",
        "# Plot 1: Residuals vs Fitted\n",
        "fitted = fe_entity_results.fitted_values\n",
        "axes[0, 0].scatter(fitted, residuals, alpha=0.5)\n",
        "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
        "axes[0, 0].set_xlabel('Fitted Values')\n",
        "axes[0, 0].set_ylabel('Residuals')\n",
        "axes[0, 0].set_title('Residuals vs Fitted Values')\n",
        "\n",
        "# Add lowess smoothing line to detect patterns\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "lowess_result = lowess(residuals.values, fitted.values, frac=0.2)\n",
        "axes[0, 0].plot(lowess_result[:, 0], lowess_result[:, 1], 'g-', label='Lowess')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Plot 2: Q-Q plot\n",
        "sm.qqplot(residuals, line='s', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Q-Q Plot')\n",
        "\n",
        "# Plot 3: Histogram of residuals with normal overlay\n",
        "axes[1, 0].hist(residuals, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].set_xlabel('Residuals')\n",
        "axes[1, 0].set_ylabel('Density')\n",
        "axes[1, 0].set_title('Distribution of Residuals')\n",
        "\n",
        "# Add normal distribution overlay using scipy\n",
        "x = np.linspace(residuals.min(), residuals.max(), 100)\n",
        "axes[1, 0].plot(x, stats.norm.pdf(x, residuals.mean(), residuals.std()), \n",
        "                'r-', linewidth=2, label='Normal')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Plot 4: Residuals over time (for a sample of entities)\n",
        "sample_entities = np.random.choice(data['entity_id'].unique(), 5, replace=False)\n",
        "for entity in sample_entities:\n",
        "    entity_mask = data_panel.index.get_level_values(0) == entity\n",
        "    entity_residuals = residuals[entity_mask]\n",
        "    time_values = data_panel.index.get_level_values(1)[entity_mask]\n",
        "    axes[1, 1].plot(time_values, entity_residuals, marker='o', \n",
        "                    label=f'Firm {entity}', alpha=0.7)\n",
        "\n",
        "axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
        "axes[1, 1].set_xlabel('Time')\n",
        "axes[1, 1].set_ylabel('Residuals')\n",
        "axes[1, 1].set_title('Residuals Over Time')\n",
        "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional diagnostic statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"RESIDUAL DIAGNOSTICS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Mean of residuals: {residuals.mean():.6f} (should be ≈ 0)\")\n",
        "print(f\"Std dev of residuals: {residuals.std():.4f}\")\n",
        "print(\"\\nSee histogram plot for Jarque-Bera normality test result.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Testing for Serial Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple test for first-order serial correlation\n",
        "# Arrange residuals by entity and time\n",
        "residuals_df = pd.DataFrame({\n",
        "    'residual': residuals,\n",
        "    'entity_id': data_panel.index.get_level_values(0),\n",
        "    'time': data_panel.index.get_level_values(1)\n",
        "})\n",
        "\n",
        "# Calculate lagged residuals\n",
        "residuals_df = residuals_df.sort_values(['entity_id', 'time'])\n",
        "residuals_df['residual_lag'] = residuals_df.groupby('entity_id')['residual'].shift(1)\n",
        "\n",
        "# Remove NaN values\n",
        "residuals_df = residuals_df.dropna()\n",
        "\n",
        "# Regression of residuals on lagged residuals\n",
        "ar_test = smf.ols('residual ~ residual_lag', data=residuals_df).fit()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TEST FOR SERIAL CORRELATION (AR(1))\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Coefficient on lagged residual: {ar_test.params['residual_lag']:.4f}\")\n",
        "print(f\"T-statistic: {ar_test.tvalues['residual_lag']:.4f}\")\n",
        "print(f\"P-value: {ar_test.pvalues['residual_lag']:.4f}\")\n",
        "print(f\"\\nConclusion: {'Evidence of serial correlation' if ar_test.pvalues['residual_lag'] < 0.05 else 'No significant serial correlation detected'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"NOTE ON WOOLDRIDGE TEST\")\n",
        "print(\"=\" * 80)\n",
        "print(\"The AR(1) test above is illustrative but ignores FE estimation uncertainty.\")\n",
        "print(\"For a more formal test of serial correlation in panel data, the Wooldridge\")\n",
        "print(\"test is preferred as it accounts for the panel structure properly.\")\n",
        "print(\"\\nIn Python, you can implement it as:\")\n",
        "print(\"1. Run FE regression in first differences\")\n",
        "print(\"2. Test if coefficient on lagged residuals = -0.5\")\n",
        "print(\"\\nThe null hypothesis is no first-order autocorrelation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Advanced Topics <a id='advanced'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Clustered Standard Errors\n",
        "\n",
        "Different types of clustering account for different correlation structures in the errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different standard error types\n",
        "# Need to recreate base model for different SE types\n",
        "y = data_panel['profit']\n",
        "X = data_panel[common_params]\n",
        "fe_entity = PanelOLS(y, X, entity_effects=True, time_effects=False)\n",
        "\n",
        "fe_standard = fe_entity.fit(cov_type='unadjusted')\n",
        "fe_robust = fe_entity.fit(cov_type='robust')\n",
        "fe_clustered_entity = fe_entity.fit(cov_type='clustered', cluster_entity=True)\n",
        "fe_clustered_time = fe_entity.fit(cov_type='clustered', cluster_time=True)\n",
        "\n",
        "# Two-way clustering (entity and time) - best practice for panel data\n",
        "fe_clustered_both = fe_entity.fit(cov_type='clustered', \n",
        "                                  cluster_entity=True, \n",
        "                                  cluster_time=True)\n",
        "\n",
        "# Driscoll-Kraay standard errors with appropriate bandwidth\n",
        "# Bandwidth = floor(4(T/100)^(2/9)) is a common choice\n",
        "T = data_panel.index.get_level_values(1).nunique()\n",
        "dk_bandwidth = int(np.floor(4 * (T/100)**(2/9)))\n",
        "dk_bandwidth = max(dk_bandwidth, 1)  # Ensure at least 1\n",
        "\n",
        "fe_driscoll_kraay = fe_entity.fit(cov_type='kernel', \n",
        "                                  kernel='bartlett',\n",
        "                                  bandwidth=dk_bandwidth)\n",
        "\n",
        "# Create comparison table\n",
        "se_comparison = pd.DataFrame({\n",
        "    'Standard': fe_standard.std_errors[common_params],\n",
        "    'Robust (HC)': fe_robust.std_errors[common_params],\n",
        "    'Clustered-Entity': fe_clustered_entity.std_errors[common_params],\n",
        "    'Clustered-Time': fe_clustered_time.std_errors[common_params],\n",
        "    'Two-way Clustered': fe_clustered_both.std_errors[common_params],\n",
        "    'Driscoll-Kraay': fe_driscoll_kraay.std_errors[common_params]\n",
        "})\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STANDARD ERRORS COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(se_comparison.round(4))\n",
        "print(f\"\\nNote: Driscoll-Kraay bandwidth = {dk_bandwidth} (based on T={T})\")\n",
        "print(\"\\nRatio to Standard (shows inflation factor):\")\n",
        "print((se_comparison.div(se_comparison['Standard'], axis=0)).round(2))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"WHEN TO USE EACH TYPE:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"• Standard: Only when errors are homoskedastic and independent (rare!)\")\n",
        "print(\"• Robust: Heteroskedasticity but no correlation\")\n",
        "print(\"• Clustered-Entity: Within-entity correlation over time\")\n",
        "print(\"• Clustered-Time: Common shocks affecting all entities at same time\")\n",
        "print(\"• Two-way Clustered: Both entity and time correlation (recommended default)\")\n",
        "print(\"• Driscoll-Kraay: Spatial/cross-sectional correlation + serial correlation\")\n",
        "print(\"  (bandwidth choice matters: too small → underestimate SEs, \")\n",
        "print(\"   too large → inefficient)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 First Differences Model\n",
        "\n",
        "An alternative to fixed effects for removing time-invariant unobserved heterogeneity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create first differences\n",
        "data_sorted = data.sort_values(['entity_id', 'time'])\n",
        "diff_vars = ['profit', 'investment', 'market_conditions', 'firm_size']\n",
        "\n",
        "for var in diff_vars:\n",
        "    data_sorted[f'd_{var}'] = data_sorted.groupby('entity_id')[var].diff()\n",
        "\n",
        "# Remove first time period (NaN values)\n",
        "data_fd = data_sorted.dropna()\n",
        "\n",
        "# Fit first differences model\n",
        "fd_model = smf.ols('d_profit ~ d_investment + d_market_conditions + d_firm_size - 1', \n",
        "                   data=data_fd).fit()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FIRST DIFFERENCES MODEL\")\n",
        "print(\"=\" * 80)\n",
        "print(fd_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Dynamic Panel Models (Lagged Dependent Variable)\n",
        "\n",
        "Note: Standard fixed effects are biased with lagged dependent variables. This requires more advanced methods like Arellano-Bond GMM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create lagged profit variable\n",
        "data_sorted['profit_lag'] = data_sorted.groupby('entity_id')['profit'].shift(1)\n",
        "\n",
        "# Remove observations with missing lags\n",
        "data_dynamic = data_sorted.dropna()\n",
        "\n",
        "# WARNING: This will be biased! Shown for illustration only\n",
        "print(\"=\" * 80)\n",
        "print(\"WARNING: NICKELL BIAS IN DYNAMIC PANELS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Fixed effects with lagged dependent variable is BIASED!\")\n",
        "print(\"The bias is of order 1/T and causes DOWNWARD bias (attenuation).\")\n",
        "print(\"This example shows the bias - DO NOT use for actual inference.\\n\")\n",
        "\n",
        "# Set up panel data\n",
        "data_dynamic_panel = data_dynamic.set_index(['entity_id', 'time'])\n",
        "y_dynamic = data_dynamic_panel['profit']\n",
        "X_dynamic = data_dynamic_panel[['profit_lag', 'investment', 'market_conditions', 'firm_size']]\n",
        "\n",
        "# Fit (biased) dynamic FE model\n",
        "fe_dynamic = PanelOLS(y_dynamic, X_dynamic, entity_effects=True)\n",
        "fe_dynamic_results = fe_dynamic.fit(cov_type='clustered', cluster_entity=True)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DYNAMIC PANEL MODEL (BIASED - FOR ILLUSTRATION ONLY)\")\n",
        "print(\"=\" * 80)\n",
        "print(fe_dynamic_results)\n",
        "\n",
        "# Show the expected bias\n",
        "lagged_coef = fe_dynamic_results.params['profit_lag']\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"NICKELL BIAS DEMONSTRATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Coefficient on lagged profit: {lagged_coef:.4f}\")\n",
        "print(f\"Expected bias direction: DOWNWARD (coefficient biased toward zero)\")\n",
        "print(f\"Bias magnitude: O(1/T) = O(1/{n_periods}) ≈ {1/n_periods:.2f}\")\n",
        "print(\"\\nFor valid dynamic panel estimation, use GMM methods:\")\n",
        "print(\"• Arellano-Bond (1991): First differences + lagged levels as instruments\")\n",
        "print(\"• Blundell-Bond (1998): System GMM with additional moment conditions\")\n",
        "print(\"• Python implementation: pip install pydynpd\")\n",
        "print(\"\\nExample with pydynpd (not run):\")\n",
        "print(\">>> from pydynpd import regression\")\n",
        "print(\">>> model = regression.abond('profit profit_lag investment market_conditions', data, ['entity_id', 'time'])\")\n",
        "print(\">>> model.fit()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Common Pitfalls and Solutions <a id='pitfalls'></a>\n",
        "\n",
        "### 8.1 Time-Invariant Variables\n",
        "Fixed effects models cannot estimate coefficients for time-invariant variables as they are absorbed by the entity effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a time-invariant variable (e.g., industry)\n",
        "np.random.seed(42)\n",
        "industry_map = {i: np.random.choice(['Tech', 'Finance', 'Manufacturing']) \n",
        "                for i in range(n_entities)}\n",
        "data['industry'] = data['entity_id'].map(industry_map)\n",
        "\n",
        "# Try to include it in the model\n",
        "data_with_industry = data.set_index(['entity_id', 'time'])\n",
        "X_with_industry = pd.get_dummies(data_with_industry[['investment', 'market_conditions', \n",
        "                                                     'firm_size', 'industry']], \n",
        "                                drop_first=True)\n",
        "\n",
        "fe_industry = PanelOLS(data_with_industry['profit'], X_with_industry, \n",
        "                      entity_effects=True, drop_absorbed=True)\n",
        "fe_industry_results = fe_industry.fit()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ATTEMPTING TO INCLUDE TIME-INVARIANT VARIABLE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Variables in final model:\")\n",
        "print(fe_industry_results.params.index.tolist())\n",
        "print(\"\\nNote: Industry dummies were dropped due to collinearity with entity effects!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Unbalanced Panels\n",
        "Real-world data often has missing observations. Let's simulate this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unbalanced panel with two types of missingness\n",
        "data_unbalanced = data.copy()\n",
        "\n",
        "# Type 1: Random missingness (10% of observations)\n",
        "random_drop = np.random.random(len(data)) < 0.1\n",
        "data_unbalanced_random = data[~random_drop].copy()\n",
        "\n",
        "# Type 2: Systematic attrition (some firms exit in later periods)\n",
        "# Firms with lower profits are more likely to exit\n",
        "entity_avg_profit = data.groupby('entity_id')['profit'].mean()\n",
        "bottom_quartile = entity_avg_profit.quantile(0.25)\n",
        "struggling_firms = entity_avg_profit[entity_avg_profit < bottom_quartile].index\n",
        "\n",
        "# These firms exit after period 5\n",
        "systematic_drop = (data['entity_id'].isin(struggling_firms)) & (data['time'] > 5)\n",
        "data_unbalanced_systematic = data[~systematic_drop].copy()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"UNBALANCED PANEL ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Original observations: {len(data)}\")\n",
        "print(f\"\\nRandom missingness (10%):\")\n",
        "print(f\"  Remaining observations: {len(data_unbalanced_random)}\")\n",
        "obs_per_entity_random = data_unbalanced_random.groupby('entity_id').size()\n",
        "print(f\"  Obs per entity - Min: {obs_per_entity_random.min()}, Max: {obs_per_entity_random.max()}, Mean: {obs_per_entity_random.mean():.2f}\")\n",
        "\n",
        "print(f\"\\nSystematic attrition (low-profit firms exit):\")\n",
        "print(f\"  Remaining observations: {len(data_unbalanced_systematic)}\")\n",
        "print(f\"  Number of firms that exit early: {len(struggling_firms)}\")\n",
        "obs_per_entity_systematic = data_unbalanced_systematic.groupby('entity_id').size()\n",
        "print(f\"  Obs per entity - Min: {obs_per_entity_systematic.min()}, Max: {obs_per_entity_systematic.max()}, Mean: {obs_per_entity_systematic.mean():.2f}\")\n",
        "\n",
        "# Fit FE models on both types of unbalanced panels\n",
        "results_comparison = {'Balanced': fe_entity_results.params[common_params]}\n",
        "\n",
        "for panel_type, panel_data in [('Random Missing', data_unbalanced_random),\n",
        "                               ('Systematic Attrition', data_unbalanced_systematic)]:\n",
        "    \n",
        "    panel_indexed = panel_data.set_index(['entity_id', 'time'])\n",
        "    y_unbal = panel_indexed['profit']\n",
        "    X_unbal = panel_indexed[['investment', 'market_conditions', 'firm_size']]\n",
        "    \n",
        "    fe_unbal = PanelOLS(y_unbal, X_unbal, entity_effects=True)\n",
        "    fe_unbal_results = fe_unbal.fit(cov_type='clustered', cluster_entity=True)\n",
        "    \n",
        "    results_comparison[panel_type] = fe_unbal_results.params[common_params]\n",
        "\n",
        "# Compare results\n",
        "comparison_df = pd.DataFrame(results_comparison)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COEFFICIENT ESTIMATES UNDER DIFFERENT MISSING DATA PATTERNS\")\n",
        "print(\"=\" * 80)\n",
        "print(comparison_df.round(4))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"• Random missingness: FE estimates remain unbiased (assuming MAR)\")\n",
        "print(\"• Systematic attrition: Can lead to selection bias if exit is correlated with\")\n",
        "print(\"  time-varying unobservables not captured by the fixed effects\")\n",
        "print(\"• Fixed effects help but don't fully solve selection from time-varying factors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Small T, Large N Bias\n",
        "When the number of time periods is small relative to entities, standard errors may be underestimated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small T scenario\n",
        "small_t_data = data[data['time'] < 3].copy()  # Only use first 3 time periods\n",
        "\n",
        "print(f\"Small T scenario: T={small_t_data['time'].nunique()}, N={n_entities}\")\n",
        "print(f\"Ratio T/N = {small_t_data['time'].nunique() / n_entities:.3f} (very small!)\")\n",
        "\n",
        "# Fit models with different standard error corrections\n",
        "small_t_panel = small_t_data.set_index(['entity_id', 'time'])\n",
        "y_small_t = small_t_panel['profit']\n",
        "X_small_t = small_t_panel[['investment', 'market_conditions', 'firm_size']]\n",
        "\n",
        "fe_small_t = PanelOLS(y_small_t, X_small_t, entity_effects=True)\n",
        "\n",
        "# Compare different standard error approaches\n",
        "se_types = {\n",
        "    'Standard': 'unadjusted',\n",
        "    'Clustered-Entity': ('clustered', {'cluster_entity': True}),\n",
        "    'Clustered-Both': ('clustered', {'cluster_entity': True, 'cluster_time': True}),\n",
        "    'Driscoll-Kraay': 'kernel'\n",
        "}\n",
        "\n",
        "small_t_results = {}\n",
        "for name, cov_spec in se_types.items():\n",
        "    if isinstance(cov_spec, tuple):\n",
        "        results = fe_small_t.fit(cov_type=cov_spec[0], **cov_spec[1])\n",
        "    else:\n",
        "        results = fe_small_t.fit(cov_type=cov_spec)\n",
        "    small_t_results[name] = results\n",
        "\n",
        "# Compare standard errors\n",
        "se_small_t = pd.DataFrame({\n",
        "    name: res.std_errors[common_params] \n",
        "    for name, res in small_t_results.items()\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STANDARD ERRORS WITH SMALL T\")\n",
        "print(\"=\" * 80)\n",
        "print(se_small_t.round(4))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RECOMMENDATIONS FOR SMALL T PANELS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"1. Use clustered standard errors (entity level minimum)\")\n",
        "print(\"2. Consider bias-corrected FE estimators (Arellano 2003)\")\n",
        "print(\"3. Be cautious about inference - SEs may still be underestimated\")\n",
        "print(\"4. Driscoll-Kraay SEs can help with cross-sectional dependence\")\n",
        "print(\"5. Bootstrap methods may provide better inference\")\n",
        "print(\"\\nNote: With T=3, we only have 2 time periods after demeaning,\")\n",
        "print(\"which provides very limited within-entity variation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Fixed Effects Command Cheat Sheet\n",
        "\n",
        "| Task | linearmodels | statsmodels |\n",
        "|------|--------------|-------------|\n",
        "| **Basic FE** | `PanelOLS(y, X, entity_effects=True)` | Demean manually or use dummies |\n",
        "| **Two-way FE** | `entity_effects=True, time_effects=True` | Add time dummies |\n",
        "| **Clustered SE** | `.fit(cov_type='clustered', cluster_entity=True)` | `.get_robustcov_results(cov_type='cluster')` |\n",
        "| **Random Effects** | `RandomEffects(y, X)` | Use mixed models |\n",
        "| **Compare models** | `compare({name: results, ...})` | Manual comparison |\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Model Choice**:\n",
        "   - Use fixed effects when unobserved heterogeneity is likely correlated with regressors\n",
        "   - Use Hausman test to formally test FE vs RE\n",
        "   - Consider two-way FE if time effects are important\n",
        "\n",
        "2. **Implementation**:\n",
        "   - `linearmodels` is preferred for panel data (efficient, built-in features)\n",
        "   - `statsmodels` offers flexibility but less efficient for large panels\n",
        "   - Always use clustered standard errors for valid inference\n",
        "\n",
        "3. **Diagnostics**:\n",
        "   - Check for serial correlation\n",
        "   - Examine residual plots\n",
        "   - Test whether time effects are needed\n",
        "\n",
        "4. **Common Issues**:\n",
        "   - Cannot estimate time-invariant variables\n",
        "   - Be cautious with small T (few time periods)\n",
        "   - Dynamic models require special methods (GMM)\n",
        "\n",
        "### Practice Exercises\n",
        "\n",
        "**Exercise 1: Real Data Application**\n",
        "```python\n",
        "# Download wages panel data\n",
        "from linearmodels.datasets import wage_panel\n",
        "wages_data = wage_panel.load()\n",
        "\n",
        "# Tasks:\n",
        "# 1. Estimate returns to education using FE\n",
        "# 2. Compare with pooled OLS and RE\n",
        "# 3. Test for serial correlation\n",
        "# 4. Interpret why estimates differ\n",
        "```\n",
        "\n",
        "**Exercise 2: Simulation Study**\n",
        "```python\n",
        "# Create data with known parameters\n",
        "# Vary:\n",
        "# - Correlation between fixed effects and X\n",
        "# - Number of time periods\n",
        "# - Degree of serial correlation\n",
        "# Compare bias in OLS, RE, and FE estimators\n",
        "```\n",
        "\n",
        "**Exercise 3: Advanced Topics**\n",
        "```python\n",
        "# 1. Implement a bootstrap for FE standard errors\n",
        "# 2. Compare first-differences vs fixed effects\n",
        "# 3. Explore interactive fixed effects\n",
        "# 4. Try GMM estimation for dynamic panels (pydynpd)\n",
        "```\n",
        "\n",
        "### Next Steps:\n",
        "- For dynamic panels: explore `pydynpd` package for Arellano-Bond/Blundell-Bond GMM\n",
        "- For more complex error structures: consider `linearmodels` advanced features\n",
        "- For machine learning with panel data: look into panel-aware ML methods\n",
        "- For very large panels: consider using Dask or PySpark for distributed computing\n",
        "\n",
        "### Resources:\n",
        "- linearmodels documentation: https://bashtage.github.io/linearmodels/\n",
        "- statsmodels documentation: https://www.statsmodels.org/\n",
        "- Econometric theory: Wooldridge (2010) \"Econometric Analysis of Cross Section and Panel Data\"\n",
        "- Python panel data tutorials: Check the linearmodels example gallery\n",
        "\n",
        "## Appendix: Quick Real Data Example"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}